{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparation\n",
    "\n",
    "We start by loading our generated .txt files. \n",
    "\n",
    "We are interested in also extracting timestamps such that we can compute skips and gaps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_training = []\n",
    "raw_testing = []\n",
    "for path in Path(\"data\").rglob(\"*.txt\"):\n",
    "    if \"train\" in str(path): \n",
    "        raw_training.append(path) \n",
    "    else:\n",
    "        raw_testing.append(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "[PosixPath('data/30music-200ks_train_full.4.txt'),\n PosixPath('data/30music-200ks_train_full.3.txt'),\n PosixPath('data/30music-200ks_train_full.2.txt'),\n PosixPath('data/30music-200ks_train_full.0.txt'),\n PosixPath('data/30music-200ks_train_full.1.txt')]"
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "[PosixPath('data/30music-200ks_test.4.txt'),\n PosixPath('data/30music-200ks_test.3.txt'),\n PosixPath('data/30music-200ks_test.2.txt'),\n PosixPath('data/30music-200ks_test.0.txt'),\n PosixPath('data/30music-200ks_test.1.txt')]"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We read the raw .idomaar files.\n",
    "\n",
    "This allows us to match tracks and tags for later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_tracks = {}\n",
    "with open(\"/Users/jo/Documents/ThirtyMusic/entities/tracks.idomaar\", \"r\", encoding=\"utf-8\") as source:\n",
    "    for i, line in enumerate(source):\n",
    "        _, track_id, _, response, extended = line.split(\"\\t\")\n",
    "        response = json.loads(response)\n",
    "        extended = json.loads(extended)\n",
    "        try:\n",
    "            tag_ids = [tag[\"id\"] for tag in extended[\"tags\"]]\n",
    "        except TypeError:\n",
    "            tag_ids = []\n",
    "        all_tracks[track_id] = {\"track_id\": track_id,\n",
    "                                \"duration\": response[\"duration\"],\n",
    "                                \"artist_id\": extended[\"artists\"][0][\"id\"],\n",
    "                                \"playcount\": response[\"playcount\"],\n",
    "                                \"tag_ids\": tag_ids\n",
    "                               }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_tags = []\n",
    "with open(\"/Users/jo/Documents/ThirtyMusic/entities/tags.idomaar\", \"r\", encoding=\"utf-8\") as source:\n",
    "    for i, line in enumerate(source):\n",
    "        all_tags.append(line.split(\"/t\")[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "4519105\n"
    }
   ],
   "source": [
    "tracks = []\n",
    "for track, data in all_tracks.items():\n",
    "    tracks.append(int(data[\"track_id\"]))\n",
    "\n",
    "print(len(set(tracks)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then proceed to check number of users in the respective training and test files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "Testing for 0\nUsers\n11115\n1544\nIntersection: 889 (57.57772020725389)\nItems\n218665\n20318\nIntersection: 20318 (100.0)\nArtists\n33063\n5040\nIntersection: 5040 (100.0)\n\n\nTesting for 1\nUsers\n9334\n973\nIntersection: 582 (59.815005138746145)\nItems\n207245\n15146\nIntersection: 15146 (100.0)\nArtists\n31529\n3991\nIntersection: 3991 (100.0)\n\n\nTesting for 2\nUsers\n9025\n852\nIntersection: 516 (60.56338028169014)\nItems\n200469\n14472\nIntersection: 14472 (100.0)\nArtists\n30353\n3743\nIntersection: 3743 (100.0)\n\n\nTesting for 3\nUsers\n9580\n932\nIntersection: 578 (62.01716738197425)\nItems\n216054\n16305\nIntersection: 16305 (100.0)\nArtists\n31366\n4116\nIntersection: 4116 (100.0)\n\n\nTesting for 4\nUsers\n9337\n862\nIntersection: 535 (62.064965197215784)\nItems\n210736\n14026\nIntersection: 14026 (100.0)\nArtists\n30947\n3586\nIntersection: 3586 (100.0)\n\n\n"
    }
   ],
   "source": [
    "for i in range(5):\n",
    "    print(\"Testing for\", i)\n",
    "    with open(raw_training[i], \"r\") as source:\n",
    "        train_lines = source.readlines()[1:]\n",
    "    with open(raw_testing[i], \"r\") as source:\n",
    "        test_lines = source.readlines()[1:]\n",
    "\n",
    "    # Extract all train items.\n",
    "    train_users = set()\n",
    "    train_items = set()\n",
    "    train_artists = set()\n",
    "    for line in train_lines:\n",
    "        user_id, session_id, item_id, timestamp, artist_id = line.split(\"\\t\")\n",
    "        train_users.add(user_id)\n",
    "        train_items.add(item_id)\n",
    "        train_artists.add(artist_id)\n",
    "\n",
    "    # Extract all test items.\n",
    "    test_users = set()\n",
    "    test_items = set()\n",
    "    test_artists = set()\n",
    "    for line in test_lines:\n",
    "        user_id, session_id, item_id, timestamp, artist_id = line.split(\"\\t\")\n",
    "        test_users.add(user_id)\n",
    "        test_items.add(item_id)\n",
    "        test_artists.add(artist_id)\n",
    "    \n",
    "    print(\"Users\")\n",
    "    print(len(train_users))\n",
    "    print(len(test_users))\n",
    "    print(\"Intersection: {} ({})\".format(len(train_users.intersection(test_users)), \n",
    "                                         100 * (len(train_users.intersection(test_users)) / len(test_users))))\n",
    "    print(\"Items\")\n",
    "    print(len(train_items))\n",
    "    print(len(test_items))\n",
    "    print(\"Intersection: {} ({})\".format(len(train_items.intersection(test_items)), \n",
    "                                         100 * (len(train_items.intersection(test_items)) / len(test_items))))\n",
    "    print(\"Artists\")\n",
    "    print(len(train_artists))\n",
    "    print(len(test_artists))\n",
    "    print(\"Intersection: {} ({})\".format(len(train_artists.intersection(test_artists)), \n",
    "                                         100 * (len(train_artists.intersection(test_artists)) / len(test_artists))))\n",
    "\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then proceeed to create our actual session files. This is a combination of the existing .idomaar data preprocessing file and the combine.ipynb file from the augmented lastfm1k-set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "Created user_id and session_id dictionary for data/30music-200ks_train_full.4.txt\nCreated user_id and session_id dictionary for data/30music-200ks_train_full.3.txt\nCreated user_id and session_id dictionary for data/30music-200ks_train_full.2.txt\nCreated user_id and session_id dictionary for data/30music-200ks_train_full.0.txt\nCreated user_id and session_id dictionary for data/30music-200ks_train_full.1.txt\nCreated user_id and session_id dictionary for data/30music-200ks_test.4.txt\nCreated user_id and session_id dictionary for data/30music-200ks_test.3.txt\nCreated user_id and session_id dictionary for data/30music-200ks_test.2.txt\nCreated user_id and session_id dictionary for data/30music-200ks_test.0.txt\nCreated user_id and session_id dictionary for data/30music-200ks_test.1.txt\n"
    }
   ],
   "source": [
    "def return_duration(track_id):\n",
    "    try:\n",
    "        duration = all_tracks[str(track_id)][\"duration\"]\n",
    "        if duration and duration > 0:\n",
    "            return round(duration / 1000)\n",
    "        return 0\n",
    "    except KeyError:\n",
    "        print(\"Encountered keyerror with\", track_id)\n",
    "        return 0\n",
    "\n",
    "def return_timestamp(timestamp):\n",
    "    return datetime.utcfromtimestamp(int(timestamp))\\\n",
    "                   .strftime('%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "\n",
    "allowed_fade = 5\n",
    "all_files = [(raw_training, \"training\"), (raw_testing, \"testing\")]\n",
    "for files, version in all_files:\n",
    "    # We first iterate through the training files, then testing.\n",
    "    for file_index, file_name in enumerate(files):\n",
    "        # We start by creating a users-dictionary of all users and their\n",
    "        # associated listening events in the current file. Note that the\n",
    "        # train/test split has already been compute through session-rec.\n",
    "        uids_sids = {}\n",
    "        if \"train\" in str(file_name):\n",
    "            file_index = str(file_name).split(\"full.\")[1].replace(\".txt\",\"\")\n",
    "        else:\n",
    "            file_index = str(file_name).split(\"test.\")[1].replace(\".txt\",\"\")\n",
    "\n",
    "        with open(file_name, \"r\") as source:\n",
    "            for line in source.readlines()[1:]:\n",
    "                user_id, session_id, _, _, _ = [int(e) for e in line.split(\"\\t\")]\n",
    "                if user_id in uids_sids:\n",
    "                    if session_id not in uids_sids[user_id]:\n",
    "                        uids_sids[user_id].append(session_id)\n",
    "                else:\n",
    "                    uids_sids[user_id] = [session_id]\n",
    "\n",
    "        with open(\"data/user_ids_and_session_ids_{}_{}.json\"\\\n",
    "                  .format(version, file_index), \"w\") as out:\n",
    "            json.dump(uids_sids, out)\n",
    "            print(\"Created user_id and session_id dictionary for\", file_name)    \n",
    "        \n",
    "        # We continue by extracting all sessions and their events.\n",
    "        raw_items = set()\n",
    "        sessions = {}\n",
    "        with open(file_name, \"r\") as source:\n",
    "            for line in source.readlines()[1:]:\n",
    "                _, session_id, item_id, timestamp, _ = [int(e) for e in line.split(\"\\t\")]\n",
    "                raw_items.add(item_id)\n",
    "                if session_id in sessions:\n",
    "                    if (item_id, timestamp) not in sessions[session_id]:\n",
    "                        sessions[session_id].append((item_id, timestamp))\n",
    "                else:\n",
    "                    sessions[session_id] = [(item_id, timestamp)]\n",
    "\n",
    "        # We now proceed to separating the different sessions\n",
    "        # and computing the internal skips and gaps using the\n",
    "        # duration values obtained from the .idomaar files.\n",
    "        updated_sessions = {}\n",
    "        for session_id, session_values in sessions.items():\n",
    "            prev_gap = 0\n",
    "            prev_track_duration = return_duration(session_values[0][0])\n",
    "            prev_timestamp = pd.to_datetime(return_timestamp(session_values[0][1]))\n",
    "            prev_track_missing = 0  # to match lastfm1k set.\n",
    "            \n",
    "            current_session = [(session_values[0][0], session_values[0][1])]\n",
    "            \n",
    "            for event in session_values[1:]:\n",
    "                track_id = int(event[0])\n",
    "                timestamp = int(event[1])\n",
    "\n",
    "                gap = 0\n",
    "                duration = return_duration(track_id)\n",
    "                current_track_missing = 0 # to match lastfm1k set.\n",
    "                timestamp = pd.to_datetime(return_timestamp(timestamp))\n",
    "                # Seconds between current and previous track starting.\n",
    "                difference = (timestamp - prev_timestamp).seconds\n",
    "\n",
    "                if prev_track_duration > 0:\n",
    "                    if difference >= prev_track_duration:\n",
    "                        abs_seconds = difference - prev_track_duration\n",
    "                        if abs_seconds <= allowed_fade:\n",
    "                            # In case of continuous stream.\n",
    "                            percentage_played = 1.0\n",
    "                        elif abs_seconds >= allowed_fade:\n",
    "                            # In case of a gap (i.e. pause and/or skipped track(s))\n",
    "                            percentage_played = 1.0\n",
    "                            gap = abs_seconds\n",
    "                    else:\n",
    "                        # In case of track not played in full.\n",
    "                        percentage_played = round(difference / prev_track_duration, 2)\n",
    "                        if percentage_played > 1:\n",
    "                            percentage_played = 1.0\n",
    "                else:\n",
    "                    percentage_played = 1.0\n",
    "                    gap = 0\n",
    "\n",
    "                # Update the percentage_played of the previous entry.\n",
    "                current_session[-1] = current_session[-1] + (percentage_played, \n",
    "                                                             prev_track_missing, \n",
    "                                                             prev_gap,)\n",
    "                # Append the current entry to the current session.\n",
    "                current_session.append((event[0], event[1]),)\n",
    "\n",
    "                # Update values\n",
    "                prev_gap = gap\n",
    "                prev_timestamp = timestamp\n",
    "                prev_track_duration = round(duration / 1000) if duration else 0\n",
    "                prev_track_missing = current_track_missing\n",
    "\n",
    "            # Add the last song, assume it has been completed in full and has no gap.\n",
    "            current_session[-1] = current_session[-1] + (1.0, prev_track_missing, prev_gap,)\n",
    "            \n",
    "            # Add the updated session information.\n",
    "            updated_sessions[session_id] = current_session\n",
    "        \n",
    "        with open(\"data/augmented_sessions_{}_{}.json\"\\\n",
    "                  .format(version, file_index), \"w\") as out:\n",
    "            json.dump(updated_sessions, out)\n",
    "            print(\"Finished {} set {}\".format(file_name, file_index))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Verification\n",
    "\n",
    "Before proceeding, we check that we still have the same intersection values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "Parsed  0\n216054\n16305\nIntersection: 16305 (100.0)\nParsed  1\n210736\n14026\nIntersection: 14026 (100.0)\nParsed  2\n200469\n14472\nIntersection: 14472 (100.0)\nParsed  3\n207245\n15146\nIntersection: 15146 (100.0)\nParsed  4\n218665\n20318\nIntersection: 20318 (100.0)\n"
    }
   ],
   "source": [
    "PATH = \"data/augmented_sessions_\"\n",
    "for i in range(5):\n",
    "    with open(\"{}training_{}.json\".format(PATH, i), \"r\") as source:\n",
    "        processed_training = json.load(source)\n",
    "\n",
    "    with open(\"{}testing_{}.json\".format(PATH, i), \"r\") as source:\n",
    "        processed_testing = json.load(source)\n",
    "\n",
    "    processed_training_items = set()\n",
    "    for _, values in processed_training.items():\n",
    "        for entry in values:\n",
    "            processed_training_items.add(entry[0])\n",
    "\n",
    "    processed_testing_items = set()\n",
    "    for _, values in processed_testing.items():\n",
    "        for entry in values:\n",
    "            processed_testing_items.add(entry[0])\n",
    "\n",
    "    print(\"Parsed \", i)\n",
    "    print(len(processed_training_items))\n",
    "    print(len(processed_testing_items))\n",
    "    print(\"Intersection: {} ({})\"\\\n",
    "          .format(len(processed_training_items.intersection(processed_testing_items)), \n",
    "                  100 * (len(processed_training_items.intersection(processed_testing_items))\\\n",
    "                  / len(processed_testing_items))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conversion\n",
    "\n",
    "At this point we have data of format\n",
    "\n",
    "\n",
    "```\n",
    "{\"5730\": [[1402838, 1418824941, 1.0, 0, 0], ... \n",
    "```\n",
    "\n",
    "We are interested in achieving the following files:\n",
    "\n",
    "* sessions.json \n",
    "\n",
    "`id: {\"track_idxs\": [...], \"tags_idxs\": [[...], [...],...], \"skips\": [...]})`\n",
    "\n",
    "* tags.json\n",
    "* tracks.json\n",
    "* users.json\n",
    "\n",
    "Additionally, we want to reset the ids such that they are in proper order.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "prepared_training = []\n",
    "prepared_testing = []\n",
    "for path in Path(\"data/intermediatejson\").rglob(\"*.json\"):\n",
    "    if \"augmented\" not in str(path):\n",
    "        continue\n",
    "    if \"train\" in str(path): \n",
    "        prepared_training.append(path) \n",
    "    else:\n",
    "        prepared_testing.append(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "prepared_testing = ['data/intermediatejson/augmented_sessions_testing_0.json',\n",
    "                    'data/intermediatejson/augmented_sessions_testing_1.json',\n",
    "                    'data/intermediatejson/augmented_sessions_testing_2.json',\n",
    "                    'data/intermediatejson/augmented_sessions_testing_3.json',\n",
    "                    'data/intermediatejson/augmented_sessions_testing_4.json']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "prepared_training = ['data/intermediatejson/augmented_sessions_training_0.json',\n",
    "                     'data/intermediatejson/augmented_sessions_training_1.json',\n",
    "                     'data/intermediatejson/augmented_sessions_training_2.json',\n",
    "                     'data/intermediatejson/augmented_sessions_training_3.json',\n",
    "                     'data/intermediatejson/augmented_sessions_training_4.json']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Added  training_0\nAdded  training_1\nAdded  training_2\nAdded  training_3\nAdded  training_4\nAdded  testing_0\nAdded  testing_1\nAdded  testing_2\nAdded  testing_3\nAdded  testing_4\n"
    }
   ],
   "source": [
    "updated = []\n",
    "all_prepared = prepared_training + prepared_testing\n",
    "for prepared in all_prepared:\n",
    "    version = str(prepared).split(\"_sessions_\")[1].split(\".\")[0]\n",
    "    with open(prepared, \"r\") as source:\n",
    "        data = json.load(source)\n",
    "    \n",
    "    # We start by extracting track_idxs, we then add tags_idxs\n",
    "    # before combining all skip values. At this point we have\n",
    "    # everything in the same dictionary. \n",
    "    updated_dict = {}\n",
    "    for session_id, values in data.items():\n",
    "        track_idxs = [entry[0] for entry in values]\n",
    "        tags_idxs = []\n",
    "        for track in track_idxs:\n",
    "            track_tags = all_tracks[str(track)][\"tag_ids\"]\n",
    "            track_tags += [0] * (5 - len(track_tags))\n",
    "            tags_idxs.append(track_tags)\n",
    "        skips = [entry[2] for entry in values]\n",
    "        updated_dict[session_id] = {\"raw_values\": values,\n",
    "                                    \"track_idxs\": track_idxs,\n",
    "                                    \"tags_idxs\": tags_idxs,\n",
    "                                    \"skips\": skips}\n",
    "        \n",
    "    # Add the updated dictionary to a temporary list.\n",
    "    updated.append((version, updated_dict))\n",
    "    print(\"Added \", version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "Items\n216054\n16305\nIntersection: 16305 (100.0)\nItems\n210736\n14026\nIntersection: 14026 (100.0)\nItems\n200469\n14472\nIntersection: 14472 (100.0)\nItems\n207245\n15146\nIntersection: 15146 (100.0)\nItems\n218665\n20318\nIntersection: 20318 (100.0)\n"
    }
   ],
   "source": [
    "processed_training = updated[:5]\n",
    "procsssed_testing = updated[5:]\n",
    "\n",
    "for i in range(5):\n",
    "    p_training = processed_training[i][1]\n",
    "    p_testing = procsssed_testing[i][1]\n",
    "\n",
    "    train_items = set()\n",
    "    for _, values in p_training.items():\n",
    "        for track in values[\"track_idxs\"]:\n",
    "            train_items.add(track)\n",
    "\n",
    "    test_items = set()\n",
    "    for _, values in p_testing.items():\n",
    "        for track in values[\"track_idxs\"]:\n",
    "            test_items.add(track)\n",
    "    \n",
    "    print(\"Items\")\n",
    "    print(len(train_items))\n",
    "    print(len(test_items))\n",
    "    print(\"Intersection: {} ({})\".format(len(train_items.intersection(test_items)), \n",
    "                                         100 * (len(train_items.intersection(test_items)) / len(test_items))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Extracted IDs from  training_0\nRefreshed for  training_0\nRefreshed for  testing_0\ntraining_0 54170\ntesting_0 7620\ntraining_0 216054\ntesting_0 16305\nExtracted IDs from  training_1\nRefreshed for  training_1\nRefreshed for  testing_1\ntraining_1 52441\ntesting_1 6898\ntraining_1 210736\ntesting_1 14026\nExtracted IDs from  training_2\nRefreshed for  training_2\nRefreshed for  testing_2\ntraining_2 50504\ntesting_2 6682\ntraining_2 200469\ntesting_2 14472\nExtracted IDs from  training_3\nRefreshed for  training_3\nRefreshed for  testing_3\ntraining_3 51153\ntesting_3 7108\ntraining_3 207245\ntesting_3 15146\nExtracted IDs from  training_4\nRefreshed for  training_4\nRefreshed for  testing_4\ntraining_4 52616\ntesting_4 8607\ntraining_4 218665\ntesting_4 20318\n"
    }
   ],
   "source": [
    "for i in range(5):\n",
    "    matched = []\n",
    "    refreshed_ids = []\n",
    "    for entry in updated:\n",
    "        if str(i) in entry[0]:\n",
    "            matched.append(entry)\n",
    "    \n",
    "    reordered_tracks = {}\n",
    "    reordered_tracks_id = 1\n",
    "    reordered_tags = {}\n",
    "    reordered_tags_id = 1\n",
    "    for match in matched:\n",
    "        if \"training\" in match[0]:\n",
    "            for key, value in match[1].items():\n",
    "                # Add key to reordered sessions.\n",
    "                #if key not in reordered_sessions:\n",
    "                #    reordered_sessions[key] = reordered_sessions_id\n",
    "                #    reordered_sessions_id += 1\n",
    "\n",
    "                # Iterate all tracks associated with current session.\n",
    "                for track in value[\"track_idxs\"]:\n",
    "                    if track not in reordered_tracks:\n",
    "                        reordered_tracks[track] = reordered_tracks_id\n",
    "                        reordered_tracks_id += 1\n",
    "                        \n",
    "                # Iterate all tags associated with current session.\n",
    "                for tags in value[\"tags_idxs\"]:\n",
    "                    for tag in tags:\n",
    "                        # We wish to skip 0 as this is our padding.\n",
    "                        if tag not in reordered_tags and tag != 0:\n",
    "                            reordered_tags[tag] = reordered_tags_id\n",
    "                            reordered_tags_id += 1\n",
    "                            \n",
    "            print(\"Extracted IDs from \", match[0]) \n",
    "    \n",
    "    # We now reorder the IDs in both the training and testing splits.\n",
    "    for match in matched:\n",
    "        new_data = {}\n",
    "        for key, value in match[1].items():\n",
    "            # We grab the new ID for tracks: All tracks from test exist in train.\n",
    "            new_tracks = [reordered_tracks[track] for track in value[\"track_idxs\"]]\n",
    "            new_tags = []\n",
    "            for tags in value[\"tags_idxs\"]:\n",
    "                new_tags.append([tag if tag == 0 else reordered_tags[tag] \n",
    "                                for tag in tags])\n",
    "\n",
    "            new_skips = []\n",
    "            for skip in value[\"skips\"]:\n",
    "                if skip > 0.9:\n",
    "                    new_skips.append(1)\n",
    "                else:\n",
    "                    new_skips.append(2)\n",
    "\n",
    "            new_data[int(key)] = {\"track_idxs\": new_tracks,\n",
    "                                  \"tags_idxs\": new_tags,\n",
    "                                  \"skips\": new_skips}\n",
    "        \n",
    "        with open(\"sessions_30music_{}.json\".format(match[0]), \"w\") as out:\n",
    "            json.dump(new_data, out)\n",
    "            print(\"Refreshed for \", match[0])\n",
    "        \n",
    "        refreshed_ids.append((match[0], new_data))\n",
    "\n",
    "    for version, values in refreshed_ids:\n",
    "        all_tags = set()\n",
    "        for key, entry in values.items():\n",
    "            for tags in entry[\"tags_idxs\"]:\n",
    "                for tag in tags:\n",
    "                    all_tags.add(tag)\n",
    "        print(version, len(all_tags))\n",
    "    \n",
    "    for version, values in refreshed_ids:\n",
    "        all_tracks = set()\n",
    "        for key, entry in values.items():\n",
    "            for track in entry[\"track_idxs\"]:\n",
    "                all_tracks.add(track)\n",
    "        print(version, len(all_tracks))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "Extracted IDs from  training_0\nExtracted IDs from  training_1\nExtracted IDs from  training_2\nExtracted IDs from  training_3\nExtracted IDs from  training_4\n"
    }
   ],
   "source": [
    "reordered_tracks = {}\n",
    "reordered_tracks_id = 1\n",
    "reordered_tags = {}\n",
    "reordered_tags_id = 1\n",
    "#reordered_sessions = {}\n",
    "#reordered_sessions_id = 1\n",
    "# We only read in for training as the training data holds\n",
    "# 100% of the tracks and tags the testing data does. \n",
    "for entry in updated:\n",
    "    if \"training\" in entry[0]:\n",
    "        for key, value in entry[1].items():\n",
    "            # Add key to reordered sessions.\n",
    "            #if key not in reordered_sessions:\n",
    "            #    reordered_sessions[key] = reordered_sessions_id\n",
    "            #    reordered_sessions_id += 1\n",
    "\n",
    "            # Iterate all tracks associated with current session.\n",
    "            for track in value[\"track_idxs\"]:\n",
    "                if track not in reordered_tracks:\n",
    "                    reordered_tracks[track] = reordered_tracks_id\n",
    "                    reordered_tracks_id += 1\n",
    "                    \n",
    "            # Iterate all tags associated with current session.\n",
    "            for tags in value[\"tags_idxs\"]:\n",
    "                for tag in tags:\n",
    "                    # We wish to skip 0 as this is our padding.\n",
    "                    if tag not in reordered_tags and tag != 0:\n",
    "                        reordered_tags[tag] = reordered_tags_id\n",
    "                        reordered_tags_id += 1\n",
    "                        \n",
    "        print(\"Extracted IDs from \", entry[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "Refreshed for  training_0\nRefreshed for  training_1\nRefreshed for  training_2\nRefreshed for  training_3\nRefreshed for  training_4\nRefreshed for  testing_0\nRefreshed for  testing_1\nRefreshed for  testing_2\nRefreshed for  testing_3\nRefreshed for  testing_4\n"
    }
   ],
   "source": [
    "refreshed_ids = []\n",
    "for entry in updated:\n",
    "    new_data = {}\n",
    "    for key, value in entry[1].items():\n",
    "        # We grab the new ID for tracks: All tracks from test exist in train.\n",
    "        new_tracks = [reordered_tracks[track] for track in value[\"track_idxs\"]]\n",
    "        new_tags = []\n",
    "        for tags in value[\"tags_idxs\"]:\n",
    "            new_tags.append([tag if tag == 0 else reordered_tags[tag] \n",
    "                             for tag in tags])\n",
    "\n",
    "        new_skips = []\n",
    "        for skip in value[\"skips\"]:\n",
    "            if skip > 0.9:\n",
    "                new_skips.append(1)\n",
    "            else:\n",
    "                new_skips.append(2)\n",
    "\n",
    "        new_data[int(key)] = {\"track_idxs\": new_tracks,\n",
    "                              \"tags_idxs\": new_tags,\n",
    "                              \"skips\": new_skips}\n",
    "    \n",
    "    with open(\"sessions_30music_{}.json\".format(entry[0]), \"w\") as out:\n",
    "        json.dump(new_data, out)\n",
    "        print(\"Refreshed for \", entry[0])\n",
    "    \n",
    "    refreshed_ids.append((entry[0], new_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "training_0 54170\ntesting_0 54170\ntraining_1 52441\ntesting_1 52441\ntraining_2 50504\ntesting_2 50504\ntraining_3 51153\ntesting_3 51153\ntraining_4 52616\ntesting_4 52616\n"
    }
   ],
   "source": [
    "for version, values in refreshed_ids:\n",
    "    all_tags = set()\n",
    "    for key, entry in values.items():\n",
    "        for tags in entry[\"tags_idxs\"]:\n",
    "            for tag in tags:\n",
    "                all_tags.add(tag)\n",
    "\n",
    "    print(version, len(all_tags))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "training_0 216054\ntesting_0 216054\ntraining_1 210736\ntesting_1 210736\ntraining_2 200469\ntesting_2 200469\ntraining_3 207245\ntesting_3 207245\ntraining_4 218665\ntesting_4 218665\n"
    }
   ],
   "source": [
    "for version, values in refreshed_ids:\n",
    "    all_tracks = set()\n",
    "    for key, entry in values.items():\n",
    "        for track in entry[\"track_idxs\"]:\n",
    "            all_tracks.add(track)\n",
    "\n",
    "    print(version, len(all_tracks))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "prepared_training = []\n",
    "prepared_testing = []\n",
    "for path in Path(\"data\").rglob(\"user_ids_*.json\"):\n",
    "    new_format = {}\n",
    "    with open(path, \"r\") as source:\n",
    "        data = json.load(source)\n",
    "    \n",
    "    for key, values in data.items():\n",
    "        new_format[key] = {\"session_subset\": values}\n",
    "    \n",
    "    with open(path, \"w\") as out:\n",
    "        json.dump(new_format, out)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "version": "3.7.6-final"
  },
  "orig_nbformat": 2,
  "file_extension": ".py",
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}